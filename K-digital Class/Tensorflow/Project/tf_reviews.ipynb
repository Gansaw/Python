{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. import 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. tensorflow_datasets 목록\n",
    "    + 본래 amazon_us_reviews를 다루려고 했으나 403 forbidden 메세지를 보아 더이상 tensorflow에서 지원하지 않는다는 것을 확인\n",
    "    + 이에 따라 대체 데이터셋으로 yelp_polarity_reviews를 이용하기로 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'accentdb',\n",
       " 'aeslc',\n",
       " 'aflw2k3d',\n",
       " 'ag_news_subset',\n",
       " 'ai2_arc',\n",
       " 'ai2_arc_with_ir',\n",
       " 'amazon_us_reviews',\n",
       " 'anli',\n",
       " 'answer_equivalence',\n",
       " 'arc',\n",
       " 'asqa',\n",
       " 'asset',\n",
       " 'assin2',\n",
       " 'bair_robot_pushing_small',\n",
       " 'bccd',\n",
       " 'beans',\n",
       " 'bee_dataset',\n",
       " 'beir',\n",
       " 'big_patent',\n",
       " 'bigearthnet',\n",
       " 'billsum',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'ble_wind_field',\n",
       " 'blimp',\n",
       " 'booksum',\n",
       " 'bool_q',\n",
       " 'bucc',\n",
       " 'c4',\n",
       " 'c4_wsrs',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cardiotox',\n",
       " 'cars196',\n",
       " 'cassava',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'cfq',\n",
       " 'cherry_blossoms',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar100_n',\n",
       " 'cifar10_1',\n",
       " 'cifar10_corrupted',\n",
       " 'cifar10_n',\n",
       " 'citrus_leaves',\n",
       " 'cityscapes',\n",
       " 'civil_comments',\n",
       " 'clevr',\n",
       " 'clic',\n",
       " 'clinc_oos',\n",
       " 'cmaterdb',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coco_captions',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'common_voice',\n",
       " 'conll2002',\n",
       " 'conll2003',\n",
       " 'controlled_noisy_web_labels',\n",
       " 'coqa',\n",
       " 'cos_e',\n",
       " 'cosmos_qa',\n",
       " 'covid19',\n",
       " 'covid19sum',\n",
       " 'crema_d',\n",
       " 'criteo',\n",
       " 'cs_restaurants',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'd4rl_adroit_door',\n",
       " 'd4rl_adroit_hammer',\n",
       " 'd4rl_adroit_pen',\n",
       " 'd4rl_adroit_relocate',\n",
       " 'd4rl_antmaze',\n",
       " 'd4rl_mujoco_ant',\n",
       " 'd4rl_mujoco_halfcheetah',\n",
       " 'd4rl_mujoco_hopper',\n",
       " 'd4rl_mujoco_walker2d',\n",
       " 'dart',\n",
       " 'davis',\n",
       " 'deep1b',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'dementiabank',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'diamonds',\n",
       " 'div2k',\n",
       " 'dmlab',\n",
       " 'doc_nli',\n",
       " 'dolphin_number_word',\n",
       " 'domainnet',\n",
       " 'downsampled_imagenet',\n",
       " 'drop',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'duke_ultrasound',\n",
       " 'e2e_cleaned',\n",
       " 'efron_morris75',\n",
       " 'emnist',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flic',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'forest_fires',\n",
       " 'fuss',\n",
       " 'gap',\n",
       " 'geirhos_conflict_stimuli',\n",
       " 'gem',\n",
       " 'genomics_ood',\n",
       " 'german_credit_numeric',\n",
       " 'gigaword',\n",
       " 'glove100_angular',\n",
       " 'glue',\n",
       " 'goemotions',\n",
       " 'gov_report',\n",
       " 'gpt3',\n",
       " 'gref',\n",
       " 'groove',\n",
       " 'grounded_scan',\n",
       " 'gsm8k',\n",
       " 'gtzan',\n",
       " 'gtzan_music_speech',\n",
       " 'hellaswag',\n",
       " 'higgs',\n",
       " 'hillstrom',\n",
       " 'horses_or_humans',\n",
       " 'howell',\n",
       " 'i_naturalist2017',\n",
       " 'i_naturalist2018',\n",
       " 'i_naturalist2021',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imagenet2012_fewshot',\n",
       " 'imagenet2012_multilabel',\n",
       " 'imagenet2012_real',\n",
       " 'imagenet2012_subset',\n",
       " 'imagenet_a',\n",
       " 'imagenet_lt',\n",
       " 'imagenet_pi',\n",
       " 'imagenet_r',\n",
       " 'imagenet_resized',\n",
       " 'imagenet_sketch',\n",
       " 'imagenet_v2',\n",
       " 'imagenette',\n",
       " 'imagewang',\n",
       " 'imdb_reviews',\n",
       " 'irc_disentanglement',\n",
       " 'iris',\n",
       " 'istella',\n",
       " 'kddcup99',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'laion400m',\n",
       " 'lambada',\n",
       " 'lfw',\n",
       " 'librispeech',\n",
       " 'librispeech_lm',\n",
       " 'libritts',\n",
       " 'ljspeech',\n",
       " 'lm1b',\n",
       " 'locomotion',\n",
       " 'lost_and_found',\n",
       " 'lsun',\n",
       " 'lvis',\n",
       " 'malaria',\n",
       " 'math_dataset',\n",
       " 'math_qa',\n",
       " 'mctaco',\n",
       " 'media_sum',\n",
       " 'mlqa',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'movie_lens',\n",
       " 'movie_rationales',\n",
       " 'movielens',\n",
       " 'moving_mnist',\n",
       " 'mrqa',\n",
       " 'mslr_web',\n",
       " 'mt_opt',\n",
       " 'mtnt',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'natural_instructions',\n",
       " 'natural_questions',\n",
       " 'natural_questions_open',\n",
       " 'newsroom',\n",
       " 'nsynth',\n",
       " 'nyu_depth_v2',\n",
       " 'ogbg_molpcba',\n",
       " 'omniglot',\n",
       " 'open_images_challenge2019_detection',\n",
       " 'open_images_v4',\n",
       " 'openbookqa',\n",
       " 'opinion_abstracts',\n",
       " 'opinosis',\n",
       " 'opus',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'pass',\n",
       " 'patch_camelyon',\n",
       " 'paws_wiki',\n",
       " 'paws_x_wiki',\n",
       " 'penguins',\n",
       " 'pet_finder',\n",
       " 'pg19',\n",
       " 'piqa',\n",
       " 'places365_small',\n",
       " 'placesfull',\n",
       " 'plant_leaves',\n",
       " 'plant_village',\n",
       " 'plantae_k',\n",
       " 'protein_net',\n",
       " 'q_re_cc',\n",
       " 'qa4mre',\n",
       " 'qasc',\n",
       " 'quac',\n",
       " 'quality',\n",
       " 'quickdraw_bitmap',\n",
       " 'race',\n",
       " 'radon',\n",
       " 'reddit',\n",
       " 'reddit_disentanglement',\n",
       " 'reddit_tifu',\n",
       " 'ref_coco',\n",
       " 'resisc45',\n",
       " 'rlu_atari',\n",
       " 'rlu_atari_checkpoints',\n",
       " 'rlu_atari_checkpoints_ordered',\n",
       " 'rlu_control_suite',\n",
       " 'rlu_dmlab_explore_object_rewards_few',\n",
       " 'rlu_dmlab_explore_object_rewards_many',\n",
       " 'rlu_dmlab_rooms_select_nonmatching_object',\n",
       " 'rlu_dmlab_rooms_watermaze',\n",
       " 'rlu_dmlab_seekavoid_arena01',\n",
       " 'rlu_locomotion',\n",
       " 'rlu_rwrl',\n",
       " 'robomimic_mg',\n",
       " 'robomimic_mh',\n",
       " 'robomimic_ph',\n",
       " 'robonet',\n",
       " 'robosuite_panda_pick_place_can',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 's3o4d',\n",
       " 'salient_span_wikipedia',\n",
       " 'samsum',\n",
       " 'savee',\n",
       " 'scan',\n",
       " 'scene_parse150',\n",
       " 'schema_guided_dialogue',\n",
       " 'sci_tail',\n",
       " 'scicite',\n",
       " 'scientific_papers',\n",
       " 'scrolls',\n",
       " 'sentiment140',\n",
       " 'shapes3d',\n",
       " 'sift1m',\n",
       " 'simpte',\n",
       " 'siscore',\n",
       " 'smallnorb',\n",
       " 'smartwatch_gestures',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'speech_commands',\n",
       " 'spoken_digit',\n",
       " 'squad',\n",
       " 'squad_question_generation',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'star_cfq',\n",
       " 'starcraft_video',\n",
       " 'stl10',\n",
       " 'story_cloze',\n",
       " 'summscreen',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'symmetric_solids',\n",
       " 'tao',\n",
       " 'tatoeba',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tedlium',\n",
       " 'tf_flowers',\n",
       " 'the300w_lp',\n",
       " 'tiny_shakespeare',\n",
       " 'titanic',\n",
       " 'trec',\n",
       " 'trivia_qa',\n",
       " 'tydi_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'unified_qa',\n",
       " 'universal_dependencies',\n",
       " 'unnatural_instructions',\n",
       " 'user_libri_audio',\n",
       " 'user_libri_text',\n",
       " 'vctk',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc',\n",
       " 'voxceleb',\n",
       " 'voxforge',\n",
       " 'waymo_open_dataset',\n",
       " 'web_graph',\n",
       " 'web_nlg',\n",
       " 'web_questions',\n",
       " 'webvid',\n",
       " 'wider_face',\n",
       " 'wiki40b',\n",
       " 'wiki_auto',\n",
       " 'wiki_bio',\n",
       " 'wiki_dialog',\n",
       " 'wiki_table_questions',\n",
       " 'wiki_table_text',\n",
       " 'wikiann',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wikipedia_toxicity_subtypes',\n",
       " 'wine_quality',\n",
       " 'winogrande',\n",
       " 'wit',\n",
       " 'wit_kaggle',\n",
       " 'wmt13_translate',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'wordnet',\n",
       " 'wsc273',\n",
       " 'xnli',\n",
       " 'xquad',\n",
       " 'xsum',\n",
       " 'xtreme_pawsx',\n",
       " 'xtreme_pos',\n",
       " 'xtreme_s',\n",
       " 'xtreme_xnli',\n",
       " 'yahoo_ltrc',\n",
       " 'yelp_polarity_reviews',\n",
       " 'yes_no',\n",
       " 'youtube_vis']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. train 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds_name = 'yelp_polarity_reviews'\n",
    "\n",
    "train_dataset, train_info = tfds.load(name=tfds_name, split=tfds.Split.TRAIN, with_info=True)\n",
    "test_dataset, test_info = tfds.load(name=tfds_name, split=tfds.Split.TEST, with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.filter(lambda data: data['text'] is not None)\n",
    "test_dataset = test_dataset.filter(lambda data: data['text'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_FilterDataset element_spec={'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'text': TensorSpec(shape=(), dtype=tf.string, name=None)}>\n",
      "<_FilterDataset element_spec={'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'text': TensorSpec(shape=(), dtype=tf.string, name=None)}>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='yelp_polarity_reviews',\n",
      "    full_name='yelp_polarity_reviews/0.2.0',\n",
      "    description=\"\"\"\n",
      "    Large Yelp Review Dataset.\n",
      "    This is a dataset for binary sentiment classification. We provide a set of 560,000 highly polar yelp reviews for training, and 38,000 for testing. \n",
      "    ORIGIN\n",
      "    The Yelp reviews dataset consists of reviews from Yelp. It is extracted\n",
      "    from the Yelp Dataset Challenge 2015 data. For more information, please\n",
      "    refer to http://www.yelp.com/dataset\n",
      "    \n",
      "    The Yelp reviews polarity dataset is constructed by\n",
      "    Xiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\n",
      "    It is first used as a text classification benchmark in the following paper:\n",
      "    Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks\n",
      "    for Text Classification. Advances in Neural Information Processing Systems 28\n",
      "    (NIPS 2015).\n",
      "    \n",
      "    \n",
      "    DESCRIPTION\n",
      "    \n",
      "    The Yelp reviews polarity dataset is constructed by considering stars 1 and 2\n",
      "    negative, and 3 and 4 positive. For each polarity 280,000 training samples and\n",
      "    19,000 testing samples are take randomly. In total there are 560,000 trainig\n",
      "    samples and 38,000 testing samples. Negative polarity is class 1,\n",
      "    and positive class 2.\n",
      "    \n",
      "    The files train.csv and test.csv contain all the training samples as\n",
      "    comma-sparated values. There are 2 columns in them, corresponding to class\n",
      "    index (1 and 2) and review text. The review texts are escaped using double\n",
      "    quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\").\n",
      "    New lines are escaped by a backslash followed with an \"n\" character,\n",
      "    that is \"\n",
      "    \".\n",
      "    \"\"\",\n",
      "    homepage='https://course.fast.ai/datasets',\n",
      "    data_path='C:\\\\Users\\\\user\\\\tensorflow_datasets\\\\yelp_polarity_reviews\\\\0.2.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=158.67 MiB,\n",
      "    dataset_size=435.14 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=38000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=560000, num_shards=4>,\n",
      "    },\n",
      "    citation=\"\"\"@article{zhangCharacterlevelConvolutionalNetworks2015,\n",
      "      archivePrefix = {arXiv},\n",
      "      eprinttype = {arxiv},\n",
      "      eprint = {1509.01626},\n",
      "      primaryClass = {cs},\n",
      "      title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},\n",
      "      abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},\n",
      "      journal = {arXiv:1509.01626 [cs]},\n",
      "      author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n",
      "      month = sep,\n",
      "      year = {2015},\n",
      "    }\"\"\",\n",
      ")\n",
      "tfds.core.DatasetInfo(\n",
      "    name='yelp_polarity_reviews',\n",
      "    full_name='yelp_polarity_reviews/0.2.0',\n",
      "    description=\"\"\"\n",
      "    Large Yelp Review Dataset.\n",
      "    This is a dataset for binary sentiment classification. We provide a set of 560,000 highly polar yelp reviews for training, and 38,000 for testing. \n",
      "    ORIGIN\n",
      "    The Yelp reviews dataset consists of reviews from Yelp. It is extracted\n",
      "    from the Yelp Dataset Challenge 2015 data. For more information, please\n",
      "    refer to http://www.yelp.com/dataset\n",
      "    \n",
      "    The Yelp reviews polarity dataset is constructed by\n",
      "    Xiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\n",
      "    It is first used as a text classification benchmark in the following paper:\n",
      "    Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks\n",
      "    for Text Classification. Advances in Neural Information Processing Systems 28\n",
      "    (NIPS 2015).\n",
      "    \n",
      "    \n",
      "    DESCRIPTION\n",
      "    \n",
      "    The Yelp reviews polarity dataset is constructed by considering stars 1 and 2\n",
      "    negative, and 3 and 4 positive. For each polarity 280,000 training samples and\n",
      "    19,000 testing samples are take randomly. In total there are 560,000 trainig\n",
      "    samples and 38,000 testing samples. Negative polarity is class 1,\n",
      "    and positive class 2.\n",
      "    \n",
      "    The files train.csv and test.csv contain all the training samples as\n",
      "    comma-sparated values. There are 2 columns in them, corresponding to class\n",
      "    index (1 and 2) and review text. The review texts are escaped using double\n",
      "    quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\").\n",
      "    New lines are escaped by a backslash followed with an \"n\" character,\n",
      "    that is \"\n",
      "    \".\n",
      "    \"\"\",\n",
      "    homepage='https://course.fast.ai/datasets',\n",
      "    data_path='C:\\\\Users\\\\user\\\\tensorflow_datasets\\\\yelp_polarity_reviews\\\\0.2.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=158.67 MiB,\n",
      "    dataset_size=435.14 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=38000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=560000, num_shards=4>,\n",
      "    },\n",
      "    citation=\"\"\"@article{zhangCharacterlevelConvolutionalNetworks2015,\n",
      "      archivePrefix = {arXiv},\n",
      "      eprinttype = {arxiv},\n",
      "      eprint = {1509.01626},\n",
      "      primaryClass = {cs},\n",
      "      title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},\n",
      "      abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},\n",
      "      journal = {arXiv:1509.01626 [cs]},\n",
      "      author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n",
      "      month = sep,\n",
      "      year = {2015},\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(train_info)\n",
    "print(test_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. seed(난수)를 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 42\n",
    "tf.random.set_seed(value)\n",
    "np.random.seed(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. train_dataset을 list로 전환하여 train_x, train_y를 만든다.\n",
    "    test_dataset에도 마찬가지로 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label']\n"
     ]
    }
   ],
   "source": [
    "# 열 정보 확인\n",
    "column_names = list(train_info.features.keys())\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b\"The Groovy P. and I ventured to his old stomping grounds for lunch today.  The '5 and Diner' on 16th St and Colter left me with little to ask for.  Before coming here I had a preconceived notion that 5 & Diners were dirty and nasty. Not the case at all.\\\\n\\\\nWe walk in and let the waitress know we want to sit outside (since it's so nice and they had misters).  We get two different servers bringing us stuff (talk about service) and I ask the one waitress for recommendations.  I didn't listen to her, of course, and ordered the Southwestern Burger w/ coleslaw and started with a nice stack of rings.\\\\n\\\\nThe Onion Rings were perfectly cooked.  They looked like they were prepackaged, but they were very crispy and I could actually bite through the onion without pulling the entire thing out (don't you hate that?!!!)\\\\n\\\\nThe Southwestern Burger was order Medium Rare and was cooked accordingly.  Soft, juicy, and pink with a nice crispy browned outer layer that can only be achieved on a well used grill.  The creaminess of the chipotle mayo paired beautifully with the green chiles.  Unfortunately, because I ate too many onion rings, I couldn't finish my burger.  What a shame!\\\\n\\\\nThe Coleslaw was just how I like it.  It's hard to find a really good coleslaw.  I prefer mine to be slightly sweet, not sour.  Too much vinegar in slaw ruins it in my opinion.  This slaw had the perfect marriage of mayo, vinegar, and sugar. Not to mention carrots...\\\\n\\\\nMy experience here was great!  The servers were top notch and kept my water full the entire time and actually chatted with us for a few minutes.\\\\n\\\\nThere is an artist guy named Ross who has been there every day for 5393 days straight. No, not an employee.  He goes there and does his art! He hasn't missed a SINGLE day!!! That's like... 15 years! So if you wanna seem to be 'in the know' ask where Ross is... They'll be able to tell you.\\\\n\\\\nTime for a nap!\"\n",
      " b\"Mediocre burgers - if you are in the area and want a fast food burger, Fatburger is  a better bet than Wendy's. But it is nothing to go out of your way for.\"\n",
      " b'Not at all impressed...our server was not very happy to be there...food was very sub-par and it was way to crowded. Not the good kind I crowded where you feel like \\\\\"\"wow this is great it must be busy because the food is so great..\\\\\"\" But the type of crowded where you feel a fight may break out. Also, if the chocolate fountain is their golden gem...why is it ok for people to dip the strawberry,lick off the chocolate and re dip it right then and there...absolutely disgusting! I Waited almost 10 minutes to dip...saw that and was immediately turned away...just saying...Never again!'\n",
      " b\"I wish I would have read Megan P's review before I decided to cancel my dinner reservations because I was offered a COMP dinner at Beso.  \\\\n\\\\nTwo of my girlfriends and I were walking around Fashion Show Mall as a promoter approaches us to ask if we'd like to go to Eve Nightclub that night.  We wanted to go to Haze and XS so we declined.  The promoter (by the way - his name is ANDRE) offers us a comp dinner at Beso Eva Longoria's restaurant at Crystals Mall.  I guess what's the catch right?  Is there a comp limit and we have to pay after we hit the limit?  No, it's a set dinner.  He sets everything up and promises he'll even walk us into Haze and XS afterwards (which I actually believed).  \\\\n\\\\nHere's a review for ANDRE:\\\\nHe tells you to call him when you're at the restaurant to get you seated.  Well - he doesn't even pick up or return text messages.  Basically, I never even see him again since he approached us at Fashion Show Mall.  He never came to give us our bottle of campagne that he promised either.  AND he never shows up to walk us into Haze or set us up for XS.  On top of that, he gave my number to other promoters (for LAVO) without letting me know.  I guess these promoters get paid by the number of girls they can pull but I've definitely dealt with more reliable promoters that actually keep their word.  \\\\n\\\\nHere's a review for the MEAL:  \\\\nWe had a party of 9 girls.  We ordered cocktails ($15) and $4 (soda/iced tea/etc).  I ordered the Ginger Cosmo and I'd say I'd definitely not order that off the menu again.  (It was highly recommended by our waitress by the way!)  I didn't think any of the drinks were that great as I sipped my friend's drinks.  \\\\n\\\\nThe meal included the following:  (Keep in mind 9 GIRLS!!!)\\\\n1.  3 TINY portions of guac and chips.  Guac was OK but I've had better and I felt like the chips were a little stale.  \\\\n2.  2 plates of caesar salad.  I mean you can't really go wrong with caesar salad but I don't think the lettuce was very fresh. \\\\n3.  2 plates of steak and potatoes.  The portion on each plate was enough to probably feed 1 person so imagine 4 to 5 girls sharing the portion for 1 person.  I mean yes it's Vegas and we want to look good in our dresses but we also want to eat so we don't get wasted.  Yes, the steak was good - the whole 4 bites that I ate!  It's literally 1 slice per person as it's 5 pieces pre-cut.\\\\n\\\\nI'm sure this meal would have been better if we actually paid to dine at Beso but to be honest promoters should just tell people that it's a comp taste sampling and to come eaten!  I was on my phone yelping the nearest fast food restaurant for us to eat more but this comp meal only takes reservations 9/9:30/10 (they just want you to go to Eve Club even if you don't want to) so it doesn't really allow time to eat meal #2 before trying to get into a club the promoter promised to walk you right in but goes MIA.  I definitely would have rather paid to eat elsewhere and I will probably never step foot into Beso again.  \\\\n\\\\nYou win this time but I hope I win in the long run with this yelp review.\"\n",
      " b'A large selection of food from all over the world. Great atmosphere and ambiance.  Quality of food is on par with a 5 star hotel. But did not have lobster and king crab that I was expecting for this kind of price.']\n",
      "[1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# train_x, train_y 만들기\n",
    "\n",
    "datalist = list(train_dataset.as_numpy_iterator())\n",
    "\n",
    "train_x = np.array([data['text'] for data in datalist])\n",
    "train_y = np.array([data['label'] for data in datalist])\n",
    "\n",
    "print(train_x[:5])\n",
    "print(train_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Was not impressed, and will not return.'\n",
      " b'I went in to purchase overalls and was treated so rudely I had to walk out even though he had the item I wanted.  I will never step foot in this placec of business again.'\n",
      " b'This place really is horrible... Every time I wind up getting convinced to go here, I always walk out feeling like my pocket has been picked. The food isn\\'t \\\\\"\"bad,\\\\\"\" but at roughly $25-$30 per person, there are SO many better places to eat. To hell with Olive Garden.\\\\n\\\\nAnd this particular one has the worst-smelling parking lot in Las Vegas... every time I drive by (to go to B.J.\\'s nearby, which is a thousand times better), it smells like a sewer main broke or something. \\\\n\\\\n2 stars only because the wait staff is friendly.'\n",
      " b'First time visit.....  enjoyed their little cheese biscuits .... Had the ribs,beef brisket,mashed taters with gravy, Mac and cheese was goooood. Very cheesy and creamy just how I like it and collard greens had a smoky taste to it, wonderful....'\n",
      " b'I\\'ll start with the good -  Price, Location, Dealertainers,  Hash House A Go-Go.   The cost to stay here was pretty cheap thanks to a special they were running through Facebook.  The location is great as it\\'s in the middle of the strip and next to some fun, older (for the strip) casinos.  The Dealertainers are awesome entertainment.  They are celebrity impersonators who not only deal your card games but also break into song and perform on stage.  Hash House A Go Go had some great food so we lucked out not having to fast food or buffet it every day.  Now the bad - The rooms and the service.  I had heard IP was recently renovated but that must not have included the rooms.  Our room was on the 12th floor - we opted for a smoking room as with these rooms you got a small balcony - which must have been the party floor as everyone who stayed there was in their early to mid 20s and had at least 4-8 people crammed in one room.   Upon entering our room the smell was pretty bad - a mix of mold and mildew and old cigs.  The carpets were stained quite badly and the decor was old and outdated.  We figured we\\'d be fine as we were only using our room to sleep in so we weren\\'t looking for much.  We called downstairs to see about getting a mini-fridge and to ask about safes in the room.  We were notified that it would cost us $10/day for a mini fridge and that there were small safes available at the front desk but we had to go down and get them ourselves.  We said thanks but no thanks.  Also, the next day when we asked for more towels we were then informed where the housekeeping desk was that that we had to \\\\\"\"get them ourselves.\\\\\"\"  Great customer service right?!  Oh we did stop to look at the pool - it\\'s a smaller size one (the one in the pics here is Harrah\\'s pool as we had a view of it from our balcony) but they had good music playing and I hear a good bar.    I don\\'t foresee us booking another stay here but I would go back and spend some money in the casino.']\n",
      "[0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# test_x, test_y 만들기\n",
    "\n",
    "datalist = list(test_dataset.as_numpy_iterator())\n",
    "\n",
    "test_x = np.array([data['text'] for data in datalist])\n",
    "test_y = np.array([data['label'] for data in datalist])\n",
    "\n",
    "print(test_x[:5])\n",
    "print(test_y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 데이터 전처리\n",
    "    + 모델에 적용시키기 위해서는 수치화된 데이터가 필요하다.\n",
    "    + 하지만 text가 존재하기 때문에 Scikit-learn의 NLP 모델을 통해 tokenizer 및 lemmatizer을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Groovy', 'and', 'I', 'ventured']\n",
      "['not', 'impressed', 'and', 'will', 'not']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# tokenizer - 문장을 단어로 token화시킨다.\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "train_x = str(train_x)\n",
    "test_x = str(test_x)\n",
    "\n",
    "train_x_tokens = word_tokenize(train_x)\n",
    "test_x_tokens = word_tokenize(test_x)\n",
    "# print(train_tokens[:5])\n",
    "# print(test_tokens[:5])\n",
    "\n",
    "# 문장부호는 불필요하기 때문에 제거한다. 그리고 b라는 불필요한 단어가 출력되어 제거하였다.\n",
    "train_x_words = [token for token in train_x_tokens if token.isalpha() and token != 'b']\n",
    "test_x_words = [token for token in test_x_tokens if token.isalpha() and token != 'b']\n",
    "\n",
    "print(train_x_words[:5])\n",
    "print(test_x_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Groovy', 'and', 'I', 'ventured']\n",
      "['not', 'impressed', 'and', 'will', 'not']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# lemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "train_x_lemmatize = [WordNetLemmatizer().lemmatize(word) for word in train_x_words]\n",
    "test_x_lemmatize = [WordNetLemmatizer().lemmatize(word) for word in test_x_words]\n",
    "\n",
    "print(train_x_lemmatize[:5])\n",
    "print(test_x_lemmatize[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. tokenizer 이후 lemmatizer을 진행하였으나 특정 단어에는 적용되지 않는 것을 확인하였다. (예: ventured) <br>\n",
    " => spacy를 사용하여 더욱 정교한 lemmatizer을 진행하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Groovy', 'and', 'I', 'venture']\n",
      "['not', 'impressed', 'and', 'will', 'not']\n"
     ]
    }
   ],
   "source": [
    "    import spacy\n",
    "\n",
    "    # 콘솔에 python -m spacy download en_core_web_sm를 입력하여 install을 해야 한다.\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    train_x_text = \" \".join(train_x_words)\n",
    "    test_x_text = \" \".join(test_x_words)\n",
    "\n",
    "    train_x_doc = nlp(train_x_text)\n",
    "    test_x_doc = nlp(test_x_text)\n",
    "    \n",
    "    # 품사 태깅을 이용하여 동사의 형태만 lemmatizer 진행\n",
    "    train_x_spacy = [token.lemma_ if token.pos_ == 'VERB' else token.text for token in train_x_doc]\n",
    "    test_x_spacy = [token.lemma_ if token.pos_ == 'VERB' else token.text for token in test_x_doc]\n",
    "\n",
    "    print(train_x_spacy[:5])\n",
    "    print(test_x_spacy[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. impressed와 같이 여전히 lemmatizer가 적용되지 않은 사레들이 존재하지만 venture의 경우에는 정상적으로 lemmatizer가 진행되었다. <br>\n",
    " 이제 본격적으로 모델을 작성하여 compile 후 fit을 적용시키기로 하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 모델을 제작하기 전에 출력 데이터를 0과 1로 이진변환한다. (감성분석)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 모델 작성 - lstm 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model 작성\n",
    "\n",
    "# model = tf.keras.Sequential()\n",
    "# model.add(tf.keras.layers.Embedding(30000, 128))\n",
    "# model.add(tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. 모델 compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model compile\n",
    "\n",
    "# opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "# model.compile(optimizer=opt, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. 모델 fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
